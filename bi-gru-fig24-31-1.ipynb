{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14836497,"sourceType":"datasetVersion","datasetId":9488831}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================================================================\n# 9)  FGSM ADVERSARIAL ROBUSTNESS TEST (inputs -> maximize MSE)\n#     - Untargeted FGSM: x_adv = clip(x + eps*sign(dL/dx))\n#     - Works for regression / sequence-to-sequence MSE\n# ================================================================\n\ndef fgsm_attack_batch(model, x, y, eps, clip_min=0.0, clip_max=1.0):\n    \"\"\"\n    FGSM for a batch.\n    x: [B,T,D]  (should be float32)\n    y: [B,T,1]\n    Returns:\n      x_adv: [B,T,D]\n      loss_clean: scalar\n      loss_adv: scalar  (computed after crafting x_adv)\n      grad_linf: scalar (max |grad|)\n    \"\"\"\n    # Ensure inputs are float32\n    x = tf.cast(x, dtype=tf.float32)\n    y = tf.cast(y, dtype=tf.float32)\n\n    x_var = tf.Variable(x)\n\n    with tf.GradientTape() as tape:\n        y_hat = model(x_var, training=False)\n        loss = tf.reduce_mean(tf.square(y - y_hat))  # MSE over batch+time\n    grad = tape.gradient(loss, x_var)\n\n    # FGSM step\n    x_adv = x_var + eps * tf.sign(grad)\n    x_adv = tf.clip_by_value(x_adv, clip_min, clip_max)\n\n    # recompute losses\n    y_hat_clean = model(x, training=False)\n    loss_clean = tf.reduce_mean(tf.square(y - y_hat_clean))\n\n    y_hat_adv = model(x_adv, training=False)\n    loss_adv = tf.reduce_mean(tf.square(y - y_hat_adv))\n\n    grad_linf = tf.reduce_max(tf.abs(grad))\n    return x_adv, loss_clean, loss_adv, grad_linf\n\n\ndef fgsm_sweep_eval(model, X, Y, eps_list, batch_size=16, clip_min=0.0, clip_max=1.0):\n    \"\"\"\n    Evaluate clean MSE and adversarial MSE for a sweep of eps values.\n    X, Y can be tf.Tensor or numpy arrays with shapes:\n      X: [N,T,D], Y: [N,T,1]\n    Returns dict with per-eps metrics.\n    \"\"\"\n    # Ensure inputs are float32\n    X = tf.cast(X, dtype=tf.float32)\n    Y = tf.cast(Y, dtype=tf.float32)\n\n    ds = tf.data.Dataset.from_tensor_slices((X, Y)).batch(batch_size)\n\n    # clean baseline once\n    clean_mse_accum, n_batches = 0.0, 0\n    for xb, yb in ds:\n        yhat = model(xb, training=False)\n        clean_mse_accum += tf.reduce_mean(tf.square(yb - yhat))\n        n_batches += 1\n    clean_mse = float(clean_mse_accum / tf.cast(n_batches, tf.float32))\n\n    results = {\"clean_mse\": clean_mse, \"per_eps\": {}}\n\n    for eps in eps_list:\n        adv_mse_accum = 0.0\n        grad_linf_accum = 0.0\n        n_batches = 0\n\n        for xb, yb in ds:\n            _, _, loss_adv, grad_linf = fgsm_attack_batch(\n                model, xb, yb, eps=float(eps), clip_min=clip_min, clip_max=clip_max\n            )\n            adv_mse_accum += loss_adv\n            grad_linf_accum += grad_linf\n            n_batches += 1\n\n        adv_mse = float(adv_mse_accum / tf.cast(n_batches, tf.float32))\n        grad_linf_mean = float(grad_linf_accum / tf.cast(n_batches, tf.float32))\n\n        results[\"per_eps\"][float(eps)] = {\n            \"adv_mse\": adv_mse,\n            \"mse_ratio_adv_over_clean\": adv_mse / (clean_mse + 1e-12),\n            \"mean_grad_linf\": grad_linf_mean,\n        }\n\n    return results\n\n\ndef craft_fgsm(model, X, Y, epsilon=0.1, relative=True, eps_floor=1e-12, batch_size=64):\n    \"\"\"\n    Create FGSM adversarial examples for the whole dataset.\n    - epsilon: if relative=True, scales by per-sample, per-feature std over time.\n               if relative=False, used as absolute value.\n    Returns: X_adv (same shape as X)\n    \"\"\"\n    # Ensure inputs are float32\n    X = tf.cast(X, dtype=tf.float32)\n    Y = tf.cast(Y, dtype=tf.float32)\n\n    N = tf.shape(X)[0]\n    Din = X.shape[-1]\n    X_adv_chunks = []\n\n    # Compute per-sample, per-feature std over time for relative scaling\n    if relative:\n        # std over time axis=1 -> shape (N, 1, Din)\n        std_t = tf.math.reduce_std(X, axis=1, keepdims=True) # X is already float32\n        eps_abs_full = epsilon * tf.maximum(std_t, eps_floor)\n    else:\n        eps_abs_full = epsilon\n\n    ds = tf.data.Dataset.from_tensor_slices((X, Y)).batch(batch_size)\n    idx = 0\n    for xb, yb in ds:\n        if relative:\n            # slice matching current batch\n            eps_b = eps_abs_full[idx: idx + tf.shape(xb)[0]]\n        else:\n            eps_b = epsilon\n        # xb and yb are already tf.float32 from the dataset created from tf.cast(X, tf.float32)\n        x_adv_b = _fgsm_batch(model, xb, yb, tf.cast(eps_b, tf.float32))\n        X_adv_chunks.append(x_adv_b.numpy())\n        idx += tf.shape(xb)[0]\n    X_adv = np.concatenate(X_adv_chunks, axis=0)\n    return X_adv\n\ndef plot_fgsm_example(model, X, Y, sample_idx=0, eps=0.02, clip_min=0.0, clip_max=1.0):\n    \"\"\"\n    Plot one sample: clean prediction vs adversarial prediction, plus input perturbation magnitude.\n    \"\"\"\n    # Ensure inputs are float32\n    x = tf.cast(X[sample_idx:sample_idx+1], dtype=tf.float32)  # [1,T,D]\n    y = tf.cast(Y[sample_idx:sample_idx+1], dtype=tf.float32)  # [1,T,1]\n\n    x_adv, loss_clean, loss_adv, grad_linf = fgsm_attack_batch(\n        model, x, y, eps=float(eps), clip_min=clip_min, clip_max=clip_max\n    )\n\n    yhat_clean = model(x, training=False).numpy().squeeze()\n    yhat_adv   = model(x_adv, training=False).numpy().squeeze()\n    ytrue      = y.numpy().squeeze()\n\n    dx = (x_adv - x).numpy().squeeze()  # [T,D]\n    dx_linf_t = np.max(np.abs(dx), axis=-1)  # [T]\n\n    plt.figure(figsize=(10,4))\n    plt.title(f\"FGSM example | idx={sample_idx} eps={eps} | clean_mse={float(loss_clean):.3e} adv_mse={float(loss_adv):.3e}\")\n    plt.plot(ytrue, label=\"target\", linewidth=2)\n    plt.plot(yhat_clean, \":\", label=\"pred (clean)\", linewidth=2)\n    plt.plot(yhat_adv, \"--\", label=\"pred (FGSM)\", linewidth=2)\n    plt.grid(True); plt.legend(); plt.show()\n\n    plt.figure(figsize=(10,3))\n    plt.title(f\"Input perturbation per time-step (Linf over features) | mean={dx_linf_t.mean():.3e}, max={dx_linf_t.max():.3e}, grad_linf={float(grad_linf):.3e}\")\n    plt.plot(dx_linf_t, linewidth=1.5)\n    plt.grid(True); plt.show()\n\n\n# ----------------- Run FGSM test on TEST split -----------------\n# eps is in your normalized [0,1] input space. Try small values first.\neps_list = [0.0, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2]\n\nfgsm_results = fgsm_sweep_eval(\n    model,\n    Xte_tf, Yte_tf,\n    eps_list=eps_list,\n    batch_size=16,\n    clip_min=0.0, clip_max=1.0\n)\n\nprint(\"\\n=== FGSM ROBUSTNESS (TEST) ===\")\nprint(f\"Clean MSE: {fgsm_results['clean_mse']:.4e}\")\nprint(\"{:>8s} {:>12s} {:>12s} {:>12s}\".format(\"eps\", \"adv_mse\", \"ratio\", \"mean|grad|inf\"))\nprint(\"-\"*54)\nfor eps in eps_list:\n    r = fgsm_results[\"per_eps\"][float(eps)]\n    print(\"{:8.3g} {:12.4e} {:12.4f} {:12.4e}\".format(\n        eps, r[\"adv_mse\"], r[\"mse_ratio_adv_over_clean\"], r[\"mean_grad_linf\"]\n    ))\n\n# Plot one illustrative sample\nplot_fgsm_example(\n    model,\n    Xte_tf.numpy(), Yte_tf.numpy(),\n    sample_idx=0,\n    eps=0.02,\n    clip_min=0.0, clip_max=1.0\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T09:11:36.712681Z","iopub.execute_input":"2026-02-14T09:11:36.712996Z","iopub.status.idle":"2026-02-14T09:17:24.704123Z","shell.execute_reply.started":"2026-02-14T09:11:36.712970Z","shell.execute_reply":"2026-02-14T09:17:24.703520Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# BLOCK 1: IMPORTS & SETUP\n# ================================================================\nimport os\nimport gc\nimport time\nimport csv\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom scipy.interpolate import InterpolatedUnivariateSpline\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Set random seed for reproducibility\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nprint(f\"TensorFlow Version: {tf.__version__}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T10:17:13.018316Z","iopub.execute_input":"2026-02-16T10:17:13.018694Z","iopub.status.idle":"2026-02-16T10:17:13.032873Z","shell.execute_reply.started":"2026-02-16T10:17:13.018666Z","shell.execute_reply":"2026-02-16T10:17:13.032160Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ================================================================\n# BLOCK 2: DATA PROCESSING HELPERS\n# ================================================================\ndef read_split(path):\n    \"\"\"Read all txt files: columns are t, in1, in2, out.\n    Removes the first data row (index 0) and header.\"\"\"\n    X, Y, T = [], [], []\n    mins, maxs = {'in1':[], 'in2':[], 'out':[]}, {'in1':[], 'in2':[], 'out':[]}\n    \n    if not os.path.exists(path):\n        print(f\"Path not found: {path}\")\n        return [], [], [], {}, {}\n\n    for fn in sorted(os.listdir(path)):\n        fp = os.path.join(path, fn)\n        if not os.path.isfile(fp):\n            continue\n            \n        try:\n            # skiprows=1 for header.\n            data = np.loadtxt(fp, unpack=True, skiprows=1)\n        except Exception as e:\n            print(f\"Error reading {fn}: {e}\")\n            continue\n\n        if data.shape[0] < 4:\n            continue\n\n        # Data assignment (assuming format: Time, In1, In2, ..., Out)\n        t   = data[0]\n        in1 = data[1]\n        in2 = data[2]\n        out = data[3] \n\n        # --- FIX: Check for NaNs ---\n        if np.any(np.isnan(in1)) or np.any(np.isnan(in2)) or np.any(np.isnan(out)):\n            continue\n        \n        # --- REQUIREMENT: Remove the first row of data ---\n        # We slice [1:] to skip the first time step\n        t   = t[1:]\n        in1 = in1[1:]\n        in2 = in2[1:]\n        out = out[1:]\n\n        X.append(np.stack([in1, in2], axis=-1))   # (L,2)\n        Y.append(out.reshape(-1,1))               # (L,1)\n        T.append(t)\n        \n        for k, v in zip(['in1','in2','out'], [in1,in2,out]):\n            mins[k].append(v.min()); maxs[k].append(v.max())\n            \n    return X, Y, T, mins, maxs\n\ndef normalize(X, Y, gmins, gmaxs):\n    \"\"\"Scale to [0,1] using global mins/maxs.\"\"\"\n    imin = np.array([gmins['in1'], gmins['in2']], dtype=np.float32)\n    imax = np.array([gmaxs['in1'], gmaxs['in2']], dtype=np.float32)\n    omin = np.float32(gmins['out'])\n    omax = np.float32(gmaxs['out'])\n\n    Xn, Yn = [], []\n    for x, y in zip(X, Y):\n        x = np.asarray(x, dtype=np.float32)\n        y = np.asarray(y, dtype=np.float32)\n        \n        denom_in = imax - imin\n        denom_in[denom_in == 0] = 1.0\n        denom_out = omax - omin\n        if denom_out == 0: denom_out = 1.0\n        \n        Xn.append((x - imin) / denom_in)\n        Yn.append((y - omin) / denom_out)\n    return Xn, Yn\n\ndef resample_split(X, Y, T, L):\n    \"\"\"Resample sequences to fixed length L using Splines.\"\"\"\n    Xr, Yr, Tr = [], [], []\n    for x, y, t in zip(X, Y, T):\n        xo = np.arange(len(x))\n        xn = np.linspace(0, len(x)-1, L)\n        \n        rx = np.stack([InterpolatedUnivariateSpline(xo, x[:,c], k=2)(xn) for c in range(2)], axis=-1)\n        ry = InterpolatedUnivariateSpline(xo, y[:,0], k=2)(xn).reshape(-1,1)\n        rt = InterpolatedUnivariateSpline(xo, t, k=2)(xn)\n        \n        Xr.append(rx)\n        Yr.append(ry)\n        Tr.append(rt)\n    return np.array(Xr, dtype=np.float32), np.array(Yr, dtype=np.float32), np.array(Tr, dtype=np.float32)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T10:17:25.158411Z","iopub.execute_input":"2026-02-16T10:17:25.158725Z","iopub.status.idle":"2026-02-16T10:17:25.172592Z","shell.execute_reply.started":"2026-02-16T10:17:25.158703Z","shell.execute_reply":"2026-02-16T10:17:25.171912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# BLOCK 3: CUSTOM VANILLA GRU CELL (FIXED SERIALIZATION)\n# ================================================================\n# Use tf.keras.utils for compatibility with older/newer TF versions\n@tf.keras.utils.register_keras_serializable(package=\"MyLayers\")\nclass VanillaGRUCell(layers.Layer):\n    \"\"\"\n    Vanilla GRU Cell from scratch.\n    Weights are exposed as self.kernel and self.recurrent_kernel.\n    \"\"\"\n    def __init__(self, units, init_min=-0.5, init_max=0.5, **kwargs):\n        super(VanillaGRUCell, self).__init__(**kwargs)\n        self.units = units\n        self.state_size = units\n        self.init_min = init_min\n        self.init_max = init_max\n\n    def build(self, input_shape):\n        input_dim = input_shape[-1]\n        \n        # Initialize weights between [-5, 5] as requested\n        initializer = tf.random_uniform_initializer(minval=self.init_min, maxval=self.init_max)\n\n        # Kernel: [Input -> Gates/Candidate]\n        # Concatenated order: z (update), r (reset), h (candidate)\n        self.kernel = self.add_weight(\n            shape=(input_dim, self.units * 3),\n            initializer=initializer,\n            name='kernel'\n        )\n        \n        # Recurrent Kernel: [Hidden -> Gates/Candidate]\n        self.recurrent_kernel = self.add_weight(\n            shape=(self.units, self.units * 3),\n            initializer=initializer,\n            name='recurrent_kernel'\n        )\n\n        self.bias = self.add_weight(\n            shape=(self.units * 3,),\n            initializer='zeros',\n            name='bias'\n        )\n        self.built = True\n\n    def call(self, inputs, states):\n        h_tm1 = states[0]  # Previous state\n\n        # 1. Linear Matrix Multiplications\n        x_k = tf.matmul(inputs, self.kernel) \n        h_k = tf.matmul(h_tm1, self.recurrent_kernel)\n        \n        # 2. Split into components (z, r, h)\n        x_z, x_r, x_h = tf.split(x_k, 3, axis=1)\n        h_z, h_r, h_h = tf.split(h_k, 3, axis=1)\n        b_z, b_r, b_h = tf.split(self.bias, 3, axis=0)\n\n        # 3. Gates\n        z = tf.nn.sigmoid(x_z + h_z + b_z) # Update gate\n        r = tf.nn.sigmoid(x_r + h_r + b_r) # Reset gate\n        \n        # 4. Candidate State (Vanilla GRU: U_h * (r . h_prev))\n        h_tilde = tf.nn.tanh(x_h + (r * h_h) + b_h)\n\n        # 5. Update State\n        h = (1 - z) * h_tm1 + z * h_tilde\n        \n        return h, [h]\n\n    def get_config(self):\n        config = super(VanillaGRUCell, self).get_config()\n        config.update({\n            \"units\": self.units,\n            \"init_min\": self.init_min,\n            \"init_max\": self.init_max\n        })\n        return config\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T10:29:21.629505Z","iopub.execute_input":"2026-02-16T10:29:21.630161Z","iopub.status.idle":"2026-02-16T10:29:21.640547Z","shell.execute_reply.started":"2026-02-16T10:29:21.630131Z","shell.execute_reply":"2026-02-16T10:29:21.639731Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# BLOCK 4: PARAMETER TRACKER CALLBACK\n# ================================================================\nclass ParameterTracker(keras.callbacks.Callback):\n    def __init__(self, filepath='param_convergence.csv'):\n        super(ParameterTracker, self).__init__()\n        self.filepath = filepath\n\n    def on_train_begin(self, logs=None):\n        with open(self.filepath, 'w', newline='') as f:\n            writer = csv.writer(f)\n            # Epoch + 10 parameters\n            headers = ['epoch'] + [f'p{i}' for i in range(10)]\n            writer.writerow(headers)\n\n    def on_epoch_end(self, epoch, logs=None):\n        try:\n            # Access weights from the first Bidirectional layer\n            # layer[0] -> Bidirectional\n            bi_layer = self.model.layers[1] # Usually layer 0 is Input, layer 1 is BiGRU\n            if not isinstance(bi_layer, layers.Bidirectional):\n                # Fallback search\n                for l in self.model.layers:\n                    if isinstance(l, layers.Bidirectional):\n                        bi_layer = l\n                        break\n            \n            all_weights = bi_layer.weights\n            \n            # Flatten the first weight matrix and take 10 elements\n            if len(all_weights) > 0:\n                w_flat = tf.reshape(all_weights[0], [-1]).numpy()\n                params = w_flat[:10]\n                \n                with open(self.filepath, 'a', newline='') as f:\n                    writer = csv.writer(f)\n                    writer.writerow([epoch] + params.tolist())\n        except Exception as e:\n            print(f\"Tracking warning: {e}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T10:29:34.495796Z","iopub.execute_input":"2026-02-16T10:29:34.496110Z","iopub.status.idle":"2026-02-16T10:29:34.504734Z","shell.execute_reply.started":"2026-02-16T10:29:34.496083Z","shell.execute_reply":"2026-02-16T10:29:34.503854Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# BLOCK 5: MODEL BUILDER (FIXED)\n# ================================================================\ndef build_custom_bi_gru(seq_len, n_inputs, hidden_units=16):\n    inp = keras.Input(shape=(seq_len, n_inputs))\n    \n    # Instantiate distinct cells for forward/backward\n    fwd_cell_1 = VanillaGRUCell(hidden_units, init_min=-0.5, init_max=0.5)\n    bwd_cell_1 = VanillaGRUCell(hidden_units, init_min=-0.5, init_max=0.5)\n    \n    # Layer 1\n    # FIX: We must explicitly set go_backwards=True for the backward layer\n    x = layers.Bidirectional(\n        layers.RNN(fwd_cell_1, return_sequences=True),\n        backward_layer=layers.RNN(bwd_cell_1, return_sequences=True, go_backwards=True)\n    )(inp)\n    \n    # Instantiate distinct cells for Layer 2\n    fwd_cell_2 = VanillaGRUCell(hidden_units, init_min=-0.5, init_max=0.5)\n    bwd_cell_2 = VanillaGRUCell(hidden_units, init_min=-0.5, init_max=0.5)\n    \n    # Layer 2\n    x = layers.Bidirectional(\n        layers.RNN(fwd_cell_2, return_sequences=True),\n        backward_layer=layers.RNN(bwd_cell_2, return_sequences=True, go_backwards=True)\n    )(x)\n\n    out = layers.Dense(1)(x)\n    \n    model = keras.Model(inp, out)\n    model.compile(\n        loss='mse',\n        optimizer=keras.optimizers.Adam(learning_rate=1e-3, clipnorm=1.0),\n        metrics=['mae', 'mse']\n    )\n    return model\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T10:29:40.049225Z","iopub.execute_input":"2026-02-16T10:29:40.049575Z","iopub.status.idle":"2026-02-16T10:29:40.056395Z","shell.execute_reply.started":"2026-02-16T10:29:40.049546Z","shell.execute_reply":"2026-02-16T10:29:40.055682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# BLOCK 6: MAIN EXECUTION & EVALUATION\n# ================================================================\n# Define paths\ntrain_dir = '/kaggle/input/bi-directional-gru-ltspice-example/Data !fig24_31_1/Train'\nval_dir   = '/kaggle/input/bi-directional-gru-ltspice-example/Data !fig24_31_1/Validation'\ntest_dir  = '/kaggle/input/bi-directional-gru-ltspice-example/Data !fig24_31_1/Test'\n\nprint(\"1. Loading and Preprocessing Data...\")\nXtr_raw, Ytr_raw, Ttr_raw, mins_tr, maxs_tr = read_split(train_dir)\nXva_raw, Yva_raw, Tva_raw, _, _             = read_split(val_dir)\nXte_raw, Yte_raw, Tte_raw, _, _             = read_split(test_dir)\n\n# Normalize\ngmins = {k: np.min(mins_tr[k]) for k in mins_tr}\ngmaxs = {k: np.max(maxs_tr[k]) for k in maxs_tr}\nXtr_n, Ytr_n = normalize(Xtr_raw, Ytr_raw, gmins, gmaxs)\nXva_n, Yva_n = normalize(Xva_raw, Yva_raw, gmins, gmaxs)\nXte_n, Yte_n = normalize(Xte_raw, Yte_raw, gmins, gmaxs)\n\n# Resample\nSEQ_LEN = 400\nXtr, Ytr, Ttr = resample_split(Xtr_n, Ytr_n, Ttr_raw, SEQ_LEN)\nXva, Yva, Tva = resample_split(Xva_n, Yva_n, Tva_raw, SEQ_LEN)\nXte, Yte, Tte = resample_split(Xte_n, Yte_n, Tte_raw, SEQ_LEN)\n\nprint(f\"   Train shape: {Xtr.shape}\")\nprint(f\"   Test shape:  {Xte.shape}\")\n\n# Build\nprint(\"\\n2. Building Model...\")\nmodel = build_custom_bi_gru(SEQ_LEN, 2, hidden_units=16)\nmodel.summary()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T10:29:49.488696Z","iopub.execute_input":"2026-02-16T10:29:49.489008Z","iopub.status.idle":"2026-02-16T10:29:49.846774Z","shell.execute_reply.started":"2026-02-16T10:29:49.488982Z","shell.execute_reply":"2026-02-16T10:29:49.846093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Callbacks\ncsv_file = 'param_convergence.csv'\ncsv_logger = ParameterTracker(csv_file)\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Train\nprint(\"\\n3. Starting Training...\")\nstart_time = time.time()\nhistory = model.fit(\n    Xtr, Ytr,\n    validation_data=(Xva, Yva),\n    epochs=20, \n    batch_size=16,\n    callbacks=[csv_logger, early_stop],\n    verbose=1\n)\ntotal_train_time = time.time() - start_time\n\n# Evaluation\nprint(\"\\n=== FINAL EVALUATION REPORT ===\")\n\n# Metrics\ntrain_loss, train_mae, train_mse = model.evaluate(Xtr, Ytr, verbose=0)\ntest_loss, test_mae, test_mse = model.evaluate(Xte, Yte, verbose=0)\n\n# Inference Time\nt0 = time.perf_counter()\n_ = model.predict(Xte[:1], verbose=0)\ninf_time_ms = (time.perf_counter() - t0) * 1000 \n\nprint(f\"{'Metric':<20} | {'Train':<12} | {'Test':<12}\")\nprint(\"-\" * 50)\nprint(f\"{'MSE':<20} | {train_mse:.5e}  | {test_mse:.5e}\")\nprint(f\"{'MAE':<20} | {train_mae:.5e}  | {test_mae:.5e}\")\nprint(\"-\" * 50)\nprint(f\"Total Train Time:       {total_train_time:.2f} s\")\nprint(f\"Inference Time (1 seq): {inf_time_ms:.2f} ms\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T10:29:59.423186Z","iopub.execute_input":"2026-02-16T10:29:59.423994Z","iopub.status.idle":"2026-02-16T10:30:24.261380Z","shell.execute_reply.started":"2026-02-16T10:29:59.423957Z","shell.execute_reply":"2026-02-16T10:30:24.260528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot 1: Parameter Convergence\nif os.path.exists(csv_file):\n    try:\n        df_params = pd.read_csv(csv_file)\n        plt.figure(figsize=(10, 5))\n        # Plot first 2 tracked parameters\n        plt.plot(df_params['epoch'], df_params['p0'], label='Weight[0]', marker='o')\n        plt.plot(df_params['epoch'], df_params['p1'], label='Weight[1]', marker='x')\n        plt.title('Parameter Convergence (Vanilla BiGRU)')\n        plt.xlabel('Epoch')\n        plt.ylabel('Weight Value')\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n    except Exception as e:\n        print(f\"Plot error: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T10:30:42.516980Z","iopub.execute_input":"2026-02-16T10:30:42.517310Z","iopub.status.idle":"2026-02-16T10:30:42.698815Z","shell.execute_reply.started":"2026-02-16T10:30:42.517282Z","shell.execute_reply":"2026-02-16T10:30:42.698033Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Plot 2: Prediction vs True\ndef plot_sample(X, Y, T, idx=0):\n    if len(X) == 0: return\n    pred = model.predict(X[idx:idx+1], verbose=0)[0]\n    plt.figure(figsize=(10, 4))\n    plt.plot(T[idx], Y[idx], label='True', linewidth=2)\n    plt.plot(T[idx], pred, label='Pred', linestyle='--', linewidth=2)\n    plt.title(f'Test Sample {idx} Prediction')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nprint(\"\\nVisualizing Test Sample...\")\nplot_sample(Xte, Yte, Tte, idx=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T10:30:46.292004Z","iopub.execute_input":"2026-02-16T10:30:46.292317Z","iopub.status.idle":"2026-02-16T10:30:46.580301Z","shell.execute_reply.started":"2026-02-16T10:30:46.292291Z","shell.execute_reply":"2026-02-16T10:30:46.579515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# BLOCK 6: EXECUTION & PHASE-PLANE PLOTTING\n# ================================================================\n# ... (Previous loading/training code remains the same) ...\n\n# 1. Load Data & Normalize (Run previous blocks if needed)\n# ...\n\n# 2. Build & Train\nprint(\"\\nStarting Training...\")\nmodel = build_custom_bi_gru(SEQ_LEN, 2, 32)\ncsv_file = 'param_convergence.csv'\ncsv_logger = ParameterTracker(csv_file)\n\nhistory = model.fit(\n    Xtr, Ytr,\n    validation_data=(Xva, Yva),\n    epochs=20, \n    batch_size=16,\n    callbacks=[csv_logger],\n    verbose=1\n)\n\n# 3. PHASE-PLANE PLOT (Param 0 vs Param 1)\ntry:\n    if os.path.exists(csv_file):\n        df = pd.read_csv(csv_file)\n        \n        if not df.empty and 'p0' in df.columns and 'p1' in df.columns:\n            plt.figure(figsize=(8, 8))\n            \n            # Scatter plot with color mapping to Epoch\n            # This shows the TRAJECTORY of convergence\n            sc = plt.scatter(df['p0'], df['p1'], c=df['epoch'], cmap='viridis', s=50, zorder=2)\n            \n            # Connect points with a line to show the path\n            plt.plot(df['p0'], df['p1'], c='gray', alpha=0.5, linestyle='--', zorder=1)\n            \n            # Mark Start and End\n            plt.plot(df['p0'].iloc[0], df['p1'].iloc[0], 'rx', markersize=12, label='Start', zorder=3)\n            plt.plot(df['p0'].iloc[-1], df['p1'].iloc[-1], 'g*', markersize=15, label='End', zorder=3)\n            \n            plt.title('Parameter Convergence Trajectory\\n(Weight 0 vs Weight 1)')\n            plt.xlabel('Parameter 0 value')\n            plt.ylabel('Parameter 1 value')\n            plt.colorbar(sc, label='Epoch')\n            plt.legend()\n            plt.grid(True)\n            plt.show()\n            \n            print(\"Plot generated: The line shows the path the weights took during optimization.\")\n        else:\n            print(\"CSV is empty or missing columns.\")\n    else:\n        print(\"CSV file not found.\")\nexcept Exception as e:\n    print(f\"Plotting error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T10:34:41.175635Z","iopub.execute_input":"2026-02-16T10:34:41.176232Z","iopub.status.idle":"2026-02-16T10:35:03.383898Z","shell.execute_reply.started":"2026-02-16T10:34:41.176191Z","shell.execute_reply":"2026-02-16T10:35:03.382944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# 1. IMPORTS & SETUP\n# ================================================================\nimport os\nimport gc\nimport csv\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom scipy.interpolate import InterpolatedUnivariateSpline\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Set global seed for reproducibility of the *sequence* of experiments\n# (Each experiment will still get a different random seed)\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nprint(f\"TensorFlow Version: {tf.__version__}\")\n\n# ================================================================\n# 2. DATA PROCESSING (Robust)\n# ================================================================\ndef read_and_process_data(base_path):\n    \"\"\"Reads, cleans, normalizes, and resamples the dataset.\"\"\"\n    train_dir = os.path.join(base_path, 'Train')\n    test_dir = os.path.join(base_path, 'Test')\n    \n    # 1. Read Raw\n    def _read(path):\n        X, Y, T = [], [], []\n        mins, maxs = {'in1':[], 'in2':[], 'out':[]}, {'in1':[], 'in2':[], 'out':[]}\n        if not os.path.exists(path): return [], [], [], {}, {}\n        \n        for fn in sorted(os.listdir(path)):\n            fp = os.path.join(path, fn)\n            if not os.path.isfile(fp): continue\n            try:\n                data = np.loadtxt(fp, unpack=True, skiprows=1)\n                if data.shape[0] < 4 or data.shape[1] < 10: continue\n                \n                # Skip first row\n                t, in1, in2, out = data[0][1:], data[1][1:], data[2][1:], data[3][1:]\n                \n                # Check NaNs\n                if not (np.all(np.isfinite(in1)) and np.all(np.isfinite(in2)) and np.all(np.isfinite(out))):\n                    continue\n                    \n                X.append(np.stack([in1, in2], axis=-1))\n                Y.append(out.reshape(-1,1))\n                T.append(t)\n                \n                mins['in1'].append(in1.min()); maxs['in1'].append(in1.max())\n                mins['in2'].append(in2.min()); maxs['in2'].append(in2.max())\n                mins['out'].append(out.min()); maxs['out'].append(out.max())\n            except: continue\n        return X, Y, T, mins, maxs\n\n    print(\"Reading Data...\")\n    Xtr, Ytr, Ttr, mins_tr, maxs_tr = _read(train_dir)\n    Xte, Yte, Tte, _, _ = _read(test_dir)\n    \n    if len(Xtr) == 0: raise ValueError(\"No training data found!\")\n\n    # 2. Normalize\n    gmins = {k: np.min(mins_tr[k]) for k in mins_tr}\n    gmaxs = {k: np.max(maxs_tr[k]) for k in maxs_tr}\n    \n    def _norm(X, Y):\n        epsilon = 1e-8\n        imin = np.array([gmins['in1'], gmins['in2']], dtype=np.float32)\n        imax = np.array([gmaxs['in1'], gmaxs['in2']], dtype=np.float32)\n        omin, omax = np.float32(gmins['out']), np.float32(gmaxs['out'])\n        \n        Xn, Yn = [], []\n        for x, y in zip(X, Y):\n            d_in = imax - imin\n            d_in[d_in < epsilon] = 1.0\n            d_out = omax - omin\n            if d_out < epsilon: d_out = 1.0\n            \n            Xn.append((x - imin) / d_in)\n            Yn.append((y - omin) / d_out)\n        return Xn, Yn\n\n    Xtr_n, Ytr_n = _norm(Xtr, Ytr)\n    Xte_n, Yte_n = _norm(Xte, Yte)\n\n    # 3. Resample\n    SEQ_LEN = 400\n    def _resample(X, Y, T):\n        Xr, Yr = [], []\n        for x, y, t in zip(X, Y, T):\n            try:\n                xo, xn = np.arange(len(x)), np.linspace(0, len(x)-1, SEQ_LEN)\n                rx = np.stack([InterpolatedUnivariateSpline(xo, x[:,c], k=2)(xn) for c in range(2)], axis=-1)\n                ry = InterpolatedUnivariateSpline(xo, y[:,0], k=2)(xn).reshape(-1,1)\n                Xr.append(rx); Yr.append(ry)\n            except: continue\n        return np.array(Xr, dtype=np.float32), np.array(Yr, dtype=np.float32)\n\n    Xtr_f, Ytr_f = _resample(Xtr_n, Ytr_n, Ttr)\n    Xte_f, Yte_f = _resample(Xte_n, Yte_n, Tte)\n    \n    # Final Safety Clean\n    Xtr_f = np.nan_to_num(Xtr_f); Ytr_f = np.nan_to_num(Ytr_f)\n    Xte_f = np.nan_to_num(Xte_f); Yte_f = np.nan_to_num(Yte_f)\n    \n    return Xtr_f, Ytr_f, Xte_f, Yte_f, SEQ_LEN\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T10:41:26.948426Z","iopub.execute_input":"2026-02-16T10:41:26.949513Z","iopub.status.idle":"2026-02-16T10:41:27.008006Z","shell.execute_reply.started":"2026-02-16T10:41:26.949427Z","shell.execute_reply":"2026-02-16T10:41:27.006466Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ================================================================\n# 3. CUSTOM MODEL COMPONENTS\n# ================================================================\n@tf.keras.utils.register_keras_serializable(package=\"MyLayers\")\nclass VanillaGRUCell(layers.Layer):\n    def __init__(self, units, **kwargs):\n        super().__init__(**kwargs)\n        self.units = units\n        self.state_size = units\n\n    def build(self, input_shape):\n        input_dim = input_shape[-1]\n        \n        # We will set the initializer in the loop later, or use Glorot here\n        # For this experiment, we rely on the kernel_initializer passed \n        # to the layer constructor if supported, or we manually set weights.\n        # But for custom cells, usually we define init in build. \n        # To support random restarts, we will use a random seed in the initializer.\n        \n        self.kernel = self.add_weight(shape=(input_dim, self.units * 3),\n                                      initializer='glorot_uniform', name='kernel')\n        self.recurrent_kernel = self.add_weight(shape=(self.units, self.units * 3),\n                                                initializer='glorot_uniform', name='recurrent_kernel')\n        self.bias = self.add_weight(shape=(self.units * 3,), initializer='zeros', name='bias')\n        self.built = True\n\n    def call(self, inputs, states):\n        h_tm1 = states[0]\n        x_k = tf.matmul(inputs, self.kernel)\n        h_k = tf.matmul(h_tm1, self.recurrent_kernel)\n        \n        x_z, x_r, x_h = tf.split(x_k, 3, axis=1)\n        h_z, h_r, h_h = tf.split(h_k, 3, axis=1)\n        b_z, b_r, b_h = tf.split(self.bias, 3, axis=0)\n\n        z = tf.nn.sigmoid(x_z + h_z + b_z)\n        r = tf.nn.sigmoid(x_r + h_r + b_r)\n        h_tilde = tf.nn.tanh(x_h + (r * h_h) + b_h)\n        h = (1 - z) * h_tm1 + z * h_tilde\n        return h, [h]\n    \n    def get_config(self):\n        return super().get_config() | {\"units\": self.units}\n\nclass ParamLogger(keras.callbacks.Callback):\n    \"\"\"Logs params to a list in memory for the loop\"\"\"\n    def __init__(self, run_id, store_dict):\n        super().__init__()\n        self.run_id = run_id\n        self.store = store_dict\n        self.store[run_id] = {'epoch':[], 'p0':[], 'p1':[]}\n\n    def on_epoch_end(self, epoch, logs=None):\n        try:\n            # Extract weights from the first layer's forward cell\n            # Path: Bidirectional -> ForwardRNN -> Cell -> Kernel\n            bi_layer = self.model.layers[1] \n            # In Keras Bidirectional, forward_layer is an RNN\n            w = bi_layer.forward_layer.cell.kernel.numpy().flatten()\n            \n            self.store[self.run_id]['epoch'].append(epoch)\n            self.store[self.run_id]['p0'].append(w[0])\n            self.store[self.run_id]['p1'].append(w[1])\n        except Exception as e:\n            print(f\"Log Error: {e}\")\n\n# ================================================================\n# 4. EXPERIMENT LOOP\n# ================================================================\ndef run_experiments(X, Y, n_runs=20, epochs=15):\n    results = {} # To store trajectories\n    \n    for i in range(n_runs):\n        print(f\"--- Starting Run {i+1}/{n_runs} ---\")\n        \n        # 1. Force Clear Session to reset randomness and memory\n        keras.backend.clear_session()\n        gc.collect()\n        \n        # 2. Set distinct seed for this run\n        # This ensures 'random_uniform' or 'glorot' produces different values\n        run_seed = 42 + i\n        tf.random.set_seed(run_seed)\n        np.random.seed(run_seed)\n        \n        # 3. Build Model\n        # Input\n        inp = keras.Input(shape=(X.shape[1], X.shape[2]))\n        \n        # IMPORTANT: To get different initializations, we rely on the \n        # tf.random.set_seed call above which affects layer build()\n        l1 = layers.Bidirectional(\n            layers.RNN(VanillaGRUCell(32), return_sequences=True),\n            backward_layer=layers.RNN(VanillaGRUCell(32), return_sequences=True, go_backwards=True)\n        )(inp)\n        \n        out = layers.Dense(1)(l1)\n        model = keras.Model(inp, out)\n        \n        # Gradient Clipping to prevent NaN on bad inits\n        model.compile(loss='mse', optimizer=keras.optimizers.Adam(1e-3, clipnorm=1.0))\n        \n        # 4. Train with Logger\n        logger = ParamLogger(run_id=f\"run_{i}\", store_dict=results)\n        \n        # Use a small subset or fewer epochs if speed is an issue\n        model.fit(X, Y, epochs=epochs, batch_size=32, verbose=0, callbacks=[logger])\n        \n        # Print final loss to ensure it didn't diverge\n        final_loss = model.evaluate(X, Y, verbose=0)\n        print(f\"Run {i+1} Final MSE: {final_loss:.5f}\")\n        \n    return results\n\n# ================================================================\n# 5. EXECUTION & PLOTTING\n# ================================================================\n\n# A. Load Data\nbase_path = '/kaggle/input/bi-directional-gru-ltspice-example/Data !fig24_31_1'\ntry:\n    Xtr, Ytr, Xte, Yte, SEQ_LEN = read_and_process_data(base_path)\n    print(f\"Data Loaded: {Xtr.shape}\")\n\n    # B. Run Loop\n    experiment_data = run_experiments(Xtr, Ytr, n_runs=20, epochs=15)\n\n    # C. Plot\n    plt.figure(figsize=(10, 8))\n    \n    # Color map\n    colors = plt.cm.jet(np.linspace(0, 1, 20))\n    \n    for idx, (run_id, data) in enumerate(experiment_data.items()):\n        p0 = data['p0']\n        p1 = data['p1']\n        \n        if len(p0) == 0: continue\n            \n        # Plot the trajectory line\n        plt.plot(p0, p1, color=colors[idx], alpha=0.6, linewidth=1.5)\n        \n        # Mark Start (Square) and End (Circle)\n        plt.scatter(p0[0], p1[0], color=colors[idx], marker='s', s=40, label='Start' if idx==0 else \"\")\n        plt.scatter(p0[-1], p1[-1], color=colors[idx], marker='o', s=60, label='End' if idx==0 else \"\")\n\n    plt.title(f'Convergence Trajectories of 20 Random Initializations\\n(Parameter 0 vs Parameter 1)')\n    plt.xlabel('Parameter 0 Value')\n    plt.ylabel('Parameter 1 Value')\n    plt.grid(True, linestyle='--', alpha=0.5)\n    \n    # Custom legend\n    from matplotlib.lines import Line2D\n    legend_elements = [Line2D([0], [0], marker='s', color='k', label='Initialization (Start)', markerfacecolor='k', markersize=8, linestyle='None'),\n                       Line2D([0], [0], marker='o', color='k', label='Converged (End)', markerfacecolor='k', markersize=10, linestyle='None')]\n    plt.legend(handles=legend_elements, loc='best')\n    \n    plt.show()\n\nexcept Exception as e:\n    print(f\"Execution Failed: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T10:41:31.495857Z","iopub.execute_input":"2026-02-16T10:41:31.496575Z","iopub.status.idle":"2026-02-16T10:44:40.500301Z","shell.execute_reply.started":"2026-02-16T10:41:31.496545Z","shell.execute_reply":"2026-02-16T10:44:40.499511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# 1. SETUP\n# ================================================================\nimport os\nimport gc\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom scipy.interpolate import InterpolatedUnivariateSpline\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Fix random seeds for reproducibility of the *experiment set*\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# ================================================================\n# 2. DATA LOADING (Robust)\n# ================================================================\ndef get_clean_data(base_path):\n    # (Simplified loader from previous steps)\n    train_dir = os.path.join(base_path, 'Train')\n    if not os.path.exists(train_dir): raise ValueError(\"Path not found!\")\n    \n    X, Y = [], []\n    for fn in sorted(os.listdir(train_dir)):\n        try:\n            fp = os.path.join(train_dir, fn)\n            data = np.loadtxt(fp, unpack=True, skiprows=1)\n            # Skip first row & check size\n            if data.shape[1] < 10: continue\n            in1, in2, out = data[1][1:], data[2][1:], data[3][1:]\n            \n            # Simple Spline Resample to 600 steps\n            L = 600\n            xo, xn = np.arange(len(in1)), np.linspace(0, len(in1)-1, L)\n            r1 = InterpolatedUnivariateSpline(xo, in1, k=2)(xn)\n            r2 = InterpolatedUnivariateSpline(xo, in2, k=2)(xn)\n            ro = InterpolatedUnivariateSpline(xo, out, k=2)(xn)\n            \n            X.append(np.stack([r1, r2], axis=-1))\n            Y.append(ro.reshape(-1,1))\n        except: continue\n        \n    X, Y = np.array(X, dtype=np.float32), np.array(Y, dtype=np.float32)\n    # Check NaNs\n    X = np.nan_to_num(X)\n    Y = np.nan_to_num(Y)\n    return X, Y\n\n# ================================================================\n# 3. MODEL DEFINITION\n# ================================================================\n@tf.keras.utils.register_keras_serializable(package=\"MyLayers\")\nclass VanillaGRUCell(layers.Layer):\n    def __init__(self, units, **kwargs):\n        super().__init__(**kwargs)\n        self.units = units\n        self.state_size = units\n\n    def build(self, input_shape):\n        # We initialize widely [-1.5, 1.5] to create distinct start points for the plot\n        init = tf.random_uniform_initializer(minval=-1.5, maxval=1.5)\n        \n        self.kernel = self.add_weight(shape=(input_shape[-1], self.units * 3),\n                                      initializer=init, name='kernel')\n        self.recurrent_kernel = self.add_weight(shape=(self.units, self.units * 3),\n                                                initializer=init, name='recurrent_kernel')\n        self.bias = self.add_weight(shape=(self.units * 3,), initializer='zeros', name='bias')\n        self.built = True\n\n    def call(self, inputs, states):\n        h_tm1 = states[0]\n        x_k = tf.matmul(inputs, self.kernel)\n        h_k = tf.matmul(h_tm1, self.recurrent_kernel)\n        \n        x_z, x_r, x_h = tf.split(x_k, 3, axis=1)\n        h_z, h_r, h_h = tf.split(h_k, 3, axis=1)\n        b_z, b_r, b_h = tf.split(self.bias, 3, axis=0)\n\n        z = tf.nn.sigmoid(x_z + h_z + b_z)\n        r = tf.nn.sigmoid(x_r + h_r + b_r)\n        h_tilde = tf.nn.tanh(x_h + (r * h_h) + b_h)\n        h = (1 - z) * h_tm1 + z * h_tilde\n        return h, [h]\n    \n    def get_config(self):\n        return super().get_config() | {\"units\": self.units}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T10:54:52.091157Z","iopub.execute_input":"2026-02-16T10:54:52.091470Z","iopub.status.idle":"2026-02-16T10:54:52.111705Z","shell.execute_reply.started":"2026-02-16T10:54:52.091443Z","shell.execute_reply":"2026-02-16T10:54:52.110885Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ================================================================\n# 4. TRAINING LOOP WITH TRACKING\n# ================================================================\nclass TrajectoryTracker(keras.callbacks.Callback):\n    def __init__(self, history_list):\n        self.history = history_list # Reference to external list\n        \n    def on_epoch_end(self, epoch, logs=None):\n        # Grab two specific weights to plot (Weight[0] and Weight[1] of first layer)\n        # We track the same two indices across all runs\n        try:\n            # Path: BiDir -> ForwardRNN -> Cell -> Kernel\n            w = self.model.layers[1].forward_layer.cell.kernel.numpy().flatten()\n            self.history.append((w[0], w[1]))\n        except: pass\n\ndef run_20_inits(X, Y):\n    all_trajectories = [] # List of lists. Each inner list is one run [(x0,y0), (x1,y1)...]\n    \n    n_runs = 20\n    print(f\"Starting {n_runs} training sessions...\")\n    \n    for i in range(n_runs):\n        # 1. Clear memory\n        keras.backend.clear_session()\n        tf.random.set_seed(42 + i) # Change seed to get new start point\n        \n        # 2. Build Model\n        inp = keras.Input(shape=(600, 2))\n        l1 = layers.Bidirectional(\n            layers.RNN(VanillaGRUCell(32), return_sequences=True),\n            backward_layer=layers.RNN(VanillaGRUCell(32), return_sequences=True, go_backwards=True)\n        )(inp)\n        out = layers.Dense(1)(l1)\n        model = keras.Model(inp, out)\n        \n        # Clipnorm is essential for the wide initialization\n        model.compile(loss='mse', optimizer=keras.optimizers.Adam(2e-3, clipnorm=1.0))\n        \n        # 3. Track weights\n        run_path = [] # Stores (w1, w2) for this specific run\n        \n        # Record INITIAL state (Epoch 0)\n        # We need to run one dummy step to initialize weights (build model)\n        model.predict(X[:1], verbose=0) \n        w_init = model.layers[1].forward_layer.cell.kernel.numpy().flatten()\n        run_path.append((w_init[0], w_init[1]))\n        \n        # Train\n        model.fit(X, Y, epochs=15, batch_size=32, verbose=0, \n                  callbacks=[TrajectoryTracker(run_path)])\n        \n        all_trajectories.append(run_path)\n        print(f\"Run {i+1}/{n_runs} complete.\")\n        \n    return all_trajectories\n\n# ================================================================\n# 5. EXECUTION & VISUALIZATION\n# ================================================================\n# Path config\nbase_path = '/kaggle/input/bi-directional-gru-ltspice-example/Data !fig24_31_1'\n\ntry:\n    # 1. Load\n    X, Y = get_clean_data(base_path)\n    print(f\"Data Shape: {X.shape}\")\n\n    # 2. Run Experiments\n    trajectories = run_20_inits(X, Y)\n\n    # 3. Plot (Matching the User's Image Style)\n    plt.figure(figsize=(10, 8))\n    \n    # Plotting loop\n    for run_data in trajectories:\n        # Unzip list of tuples [(x,y), (x,y)] -> [x,x], [y,y]\n        xs, ys = zip(*run_data)\n        \n        # Draw the trajectory line (Black lines as per paper style)\n        plt.plot(xs, ys, color='black', linewidth=1.2, alpha=0.8)\n        \n        # Draw the Equilibrium point (End of training)\n        # We use a black dot for the final point\n        plt.plot(xs[-1], ys[-1], 'ko', markersize=6) \n\n    # Formatting to match the paper\n    plt.title('Training Trajectories of 20 Random Initializations', fontsize=14)\n    plt.xlabel('Parameter X1', fontsize=12)\n    plt.ylabel('Parameter X2', fontsize=12)\n    \n    # Create a custom legend manually\n    from matplotlib.lines import Line2D\n    legend_elements = [\n        Line2D([0], [0], color='black', lw=1.5, label='Trajectory'),\n        Line2D([0], [0], marker='o', color='w', label='Equilibrium point',\n               markerfacecolor='black', markersize=8),\n    ]\n    plt.legend(handles=legend_elements, loc='best')\n    \n    plt.grid(True, linestyle='--', alpha=0.5)\n    plt.tight_layout()\n    plt.show()\n    \nexcept Exception as e:\n    print(f\"Error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T10:54:57.587681Z","iopub.execute_input":"2026-02-16T10:54:57.588406Z","iopub.status.idle":"2026-02-16T10:58:49.990302Z","shell.execute_reply.started":"2026-02-16T10:54:57.588376Z","shell.execute_reply":"2026-02-16T10:58:49.989713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# 1. IMPORTS & SETUP\n# ================================================================\nimport os\nimport gc\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom scipy.interpolate import InterpolatedUnivariateSpline\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Set seeds for reproducibility of the *experiment set*\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# ================================================================\n# 2. DATA LOADING (Robust)\n# ================================================================\ndef get_clean_data(base_path):\n    train_dir = os.path.join(base_path, 'Train')\n    if not os.path.exists(train_dir): \n        # Fallback for Kaggle paths if strictly necessary, otherwise raise\n        print(f\"Warning: Path {train_dir} not found.\")\n        return np.zeros((10, 600, 2)), np.zeros((10, 600, 1))\n    \n    X, Y = [], []\n    for fn in sorted(os.listdir(train_dir)):\n        try:\n            fp = os.path.join(train_dir, fn)\n            data = np.loadtxt(fp, unpack=True, skiprows=1)\n            if data.shape[1] < 10: continue\n            in1, in2, out = data[1][1:], data[2][1:], data[3][1:]\n            \n            # Simple Spline Resample\n            L = 600\n            xo, xn = np.arange(len(in1)), np.linspace(0, len(in1)-1, L)\n            r1 = InterpolatedUnivariateSpline(xo, in1, k=2)(xn)\n            r2 = InterpolatedUnivariateSpline(xo, in2, k=2)(xn)\n            ro = InterpolatedUnivariateSpline(xo, out, k=2)(xn)\n            \n            X.append(np.stack([r1, r2], axis=-1))\n            Y.append(ro.reshape(-1,1))\n        except: continue\n        \n    X, Y = np.array(X, dtype=np.float32), np.array(Y, dtype=np.float32)\n    return np.nan_to_num(X), np.nan_to_num(Y)\n\n# ================================================================\n# 3. MODEL DEFINITION\n# ================================================================\n@tf.keras.utils.register_keras_serializable(package=\"MyLayers\")\nclass VanillaGRUCell(layers.Layer):\n    def __init__(self, units, **kwargs):\n        super().__init__(**kwargs)\n        self.units = units\n        self.state_size = units\n\n    def build(self, input_shape):\n        # Wide initialization [-1.5, 1.5] to separate the start points visually\n        init = tf.random_uniform_initializer(minval=-1.5, maxval=1.5)\n        \n        self.kernel = self.add_weight(shape=(input_shape[-1], self.units * 3),\n                                      initializer=init, name='kernel')\n        self.recurrent_kernel = self.add_weight(shape=(self.units, self.units * 3),\n                                                initializer=init, name='recurrent_kernel')\n        self.bias = self.add_weight(shape=(self.units * 3,), initializer='zeros', name='bias')\n        self.built = True\n\n    def call(self, inputs, states):\n        h_tm1 = states[0]\n        x_k = tf.matmul(inputs, self.kernel)\n        h_k = tf.matmul(h_tm1, self.recurrent_kernel)\n        \n        x_z, x_r, x_h = tf.split(x_k, 3, axis=1)\n        h_z, h_r, h_h = tf.split(h_k, 3, axis=1)\n        b_z, b_r, b_h = tf.split(self.bias, 3, axis=0)\n\n        z = tf.nn.sigmoid(x_z + h_z + b_z)\n        r = tf.nn.sigmoid(x_r + h_r + b_r)\n        h_tilde = tf.nn.tanh(x_h + (r * h_h) + b_h)\n        h = (1 - z) * h_tm1 + z * h_tilde\n        return h, [h]\n    \n    def get_config(self):\n        return super().get_config() | {\"units\": self.units}\n\n# ================================================================\n# 4. ROBUST TRAJECTORY TRACKER\n# ================================================================\ndef get_first_kernel_weights(model):\n    \"\"\"Helper to safely find the first kernel weight regardless of layer structure.\"\"\"\n    for layer in model.layers:\n        # Check if it's bidirectional\n        if hasattr(layer, 'forward_layer'):\n            # Access the forward RNN layer\n            if hasattr(layer.forward_layer, 'cell'):\n                return layer.forward_layer.cell.kernel.numpy().flatten()\n    # Fallback: just return the first trainable weight of the model\n    if len(model.trainable_weights) > 0:\n        return model.trainable_weights[0].numpy().flatten()\n    return np.array([0.0, 0.0])\n\nclass TrajectoryTracker(keras.callbacks.Callback):\n    def __init__(self, history_list):\n        self.history = history_list \n        \n    def on_epoch_end(self, epoch, logs=None):\n        # Capture weights at end of every epoch\n        w = get_first_kernel_weights(self.model)\n        self.history.append((w[0], w[1]))\n\ndef run_20_inits(X, Y):\n    all_trajectories = [] \n    \n    n_runs = 20\n    print(f\"Starting {n_runs} training sessions...\")\n    \n    for i in range(n_runs):\n        keras.backend.clear_session()\n        tf.random.set_seed(42 + i) # Distinct seed for initialization\n        \n        # Build Model\n        inp = keras.Input(shape=(600, 2))\n        l1 = layers.Bidirectional(\n            layers.RNN(VanillaGRUCell(32), return_sequences=True),\n            backward_layer=layers.RNN(VanillaGRUCell(32), return_sequences=True, go_backwards=True)\n        )(inp)\n        out = layers.Dense(1)(l1)\n        model = keras.Model(inp, out)\n        \n        # Use simple SGD or Adam. SGD sometimes gives smoother \"curves\" for visualization.\n        model.compile(loss='mse', optimizer=keras.optimizers.Adam(5e-3, clipnorm=1.0))\n        \n        # Track weights\n        run_path = [] \n        \n        # 1. Capture INITIAL weights (Epoch 0)\n        # Force model build by passing one sample\n        model.predict(X[:1], verbose=0) \n        w_init = get_first_kernel_weights(model)\n        run_path.append((w_init[0], w_init[1]))\n        \n        # 2. Train\n        # Reduced to 10 epochs for cleaner visualization lines\n        model.fit(X, Y, epochs=15, batch_size=32, verbose=0, \n                  callbacks=[TrajectoryTracker(run_path)])\n        \n        if len(run_path) < 2:\n            print(f\"Warning: Run {i} captured too few points.\")\n            \n        all_trajectories.append(run_path)\n        print(f\"Run {i+1}/{n_runs} complete. Points captured: {len(run_path)}\")\n        \n    return all_trajectories\n\n# ================================================================\n# 5. EXECUTION & PLOTTING\n# ================================================================\nbase_path = '/kaggle/input/bi-directional-gru-ltspice-example/Data !fig24_31_1'\n\n# 1. Load Data\nX, Y = get_clean_data(base_path)\n\nif len(X) > 0:\n    # 2. Run Experiments\n    trajectories = run_20_inits(X, Y)\n\n    # 3. Plot\n    plt.figure(figsize=(10, 8))\n    \n    for i, run_data in enumerate(trajectories):\n        if len(run_data) < 2: continue\n        \n        # Unzip trajectory to X and Y coords\n        xs, ys = zip(*run_data)\n        \n        # Plot the Line (Trajectory)\n        plt.plot(xs, ys, color='black', linewidth=1.5, alpha=0.7)\n        \n        # Plot the End Point (Equilibrium)\n        plt.plot(xs[-1], ys[-1], 'ko', markersize=6, zorder=3) \n\n    # Styling to match the paper\n    plt.title('Training Trajectories of 20 Random Initializations', fontsize=14)\n    plt.xlabel('Parameter X1 (Weight 0)', fontsize=12)\n    plt.ylabel('Parameter X2 (Weight 1)', fontsize=12)\n    plt.grid(True, linestyle='--', alpha=0.5)\n    \n    # Custom Legend\n    from matplotlib.lines import Line2D\n    legend_elements = [\n        Line2D([0], [0], color='black', lw=1.5, label='Trajectory'),\n        Line2D([0], [0], marker='o', color='w', label='Equilibrium point',\n               markerfacecolor='black', markersize=8),\n    ]\n    plt.legend(handles=legend_elements, loc='upper right')\n    \n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"No data found to run experiments.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T11:18:38.494218Z","iopub.execute_input":"2026-02-16T11:18:38.495014Z","execution_failed":"2026-02-16T11:19:59.083Z"}},"outputs":[{"name":"stdout","text":"Starting 20 training sessions...\nRun 1/20 complete. Points captured: 16\nRun 2/20 complete. Points captured: 16\nRun 3/20 complete. Points captured: 16\nRun 4/20 complete. Points captured: 16\nRun 5/20 complete. Points captured: 16\nRun 6/20 complete. Points captured: 16\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}